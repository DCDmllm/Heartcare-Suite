{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"HF_DATASETS_OFFLINE\"] = \"1\"\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import ast\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import utils.my_ecg_process as ecg\n",
    "from utils.my_tokenizer import Tokenizer\n",
    "from utils.my_templates import Sentences, Choices, Reports, Predict\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, TrainingArguments, set_seed\n",
    "from safetensors.torch import load_file\n",
    "from peft import LoraConfig, TaskType\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES')}\")\n",
    "print(f\"PyTorch sees {torch.cuda.device_count()} GPU(s)\")\n",
    "print(f\"Current device index: {torch.cuda.current_device()}\")\n",
    "print(f\"Device name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = 500\n",
    "batch_size = 32\n",
    "seq_length = 500\n",
    "patch_size = 25\n",
    "latent_ratio = 0.5\n",
    "channels = 12\n",
    "codebook_size = 256\n",
    "residual_levels = 2\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "dir = f\"level_{residual_levels}_code_{codebook_size}_len_{seq_length}_ratio_{latent_ratio}\"\n",
    "ecg_tokenizer_path = f\"tokenizer/ecg_tokenizer_{dir}.pth\"\n",
    "model_path = 'phi-3'\n",
    "num = 10\n",
    "trainer_path = f\"training/{num}\"\n",
    "merged_model_path = f\"phi-3-ecg/{num}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vq_kwargs = {'residual_levels': residual_levels}\n",
    "ecg_tokenizer = Tokenizer(\n",
    "    seq_length=seq_length,\n",
    "    patch_size=patch_size,\n",
    "    latent_ratio=latent_ratio,\n",
    "    channels=channels,\n",
    "    codebook_size=codebook_size,\n",
    "    vq_kwargs=vq_kwargs\n",
    ").to(device)\n",
    "# ecg_weights = torch.load(ecg_tokenizer_path, weights_only=False)\n",
    "# ecg_tokenizer.load_state_dict(ecg_weights['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = {\n",
    "    'model_state_dict': ecg_tokenizer.state_dict(),\n",
    "}\n",
    "torch.save(tok, f\"tokenizer/tok_2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=False\n",
    ")\n",
    "\n",
    "text_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=False\n",
    ")\n",
    "\n",
    "text_tokenizer.pad_token = text_tokenizer.unk_token\n",
    "text_tokenizer.pad_token_id = text_tokenizer.convert_tokens_to_ids(text_tokenizer.pad_token)\n",
    "text_tokenizer.padding_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    config=config,\n",
    "    trust_remote_code=False,\n",
    "    torch_dtype=torch.float16,\n",
    "    attn_implementation='sdpa',\n",
    "    # device_map=\"auto\"\n",
    ").to(device)\n",
    "\n",
    "weight1 = load_file(f\"{model_path}/model-00001-of-00002.safetensors\")\n",
    "weight2 = load_file(f\"{model_path}/model-00002-of-00002.safetensors\")\n",
    "\n",
    "state_dict = {**weight1, **weight2}\n",
    "model.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "        output_dir=trainer_path,\n",
    "        logging_dir=f\"runs/{trainer_path}\",\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=500,\n",
    "        save_total_limit=2,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=500,\n",
    "        do_eval=True,\n",
    "        optim=\"adamw_torch\",\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=8,\n",
    "        per_device_eval_batch_size=4,\n",
    "        log_level=\"warning\",\n",
    "        logging_steps=100,\n",
    "        learning_rate=1e-4,\n",
    "        fp16=True,\n",
    "        bf16=False,\n",
    "        num_train_epochs=1,\n",
    "        warmup_ratio=0.1,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        report_to=\"tensorboard\",\n",
    "        seed=42,\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "        r=64,\n",
    "        lora_alpha=64,\n",
    "        lora_dropout=0.05,\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        target_modules=['k_proj', 'q_proj', 'v_proj', 'o_proj', \"gate_proj\", \"down_proj\", \"up_proj\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"data/full_data.npy\")\n",
    "total_length = int(seq_length * (1 + latent_ratio))\n",
    "start = int((X.shape[2] - total_length)/2)\n",
    "signal = X[:, :, start:start+seq_length]\n",
    "predict = X[:, :, start+seq_length:start+total_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset = TensorDataset(torch.tensor(signal, dtype=torch.float32))\n",
    "df_loader = DataLoader(df_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "new = []\n",
    "with torch.no_grad(): \n",
    "    for batch in df_loader:\n",
    "        x = batch[0].to(device)\n",
    "        ecg_tokens = ecg_tokenizer.tokenize(x).cpu().detach().tolist()\n",
    "        tokens.extend(ecg_tokens)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.read_parquet(\"data/full_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"data/full_data.npy\")\n",
    "total_length = int(seq_length * (1 + latent_ratio))\n",
    "start = int((X.shape[2] - total_length)/2)\n",
    "signal = X[:, :, start:start+seq_length]\n",
    "predict = X[:, :, start+seq_length:start+total_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset = TensorDataset(torch.tensor(signal, dtype=torch.float32))\n",
    "df_loader = DataLoader(df_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "new = []\n",
    "with torch.no_grad(): \n",
    "    for batch in df_loader:\n",
    "        x = batch[0].to(device)\n",
    "        ecg_tokens = ecg_tokenizer.tokenize(x).cpu().detach().tolist()\n",
    "        tokens.extend(ecg_tokens)\n",
    "        \n",
    "combined_df[\"tokens\"] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokens = [\"<|ecg_start|>\", \"<|ecg_end|>\", \"<|report_start|>\", \"<|report_end|>\", \"<|pred_start|>\", \"<|pred_end|>\"] + [f\"<|ecg_{i+1}|>\" for i in range(codebook_size)]\n",
    "text_tokenizer.add_tokens(new_tokens)\n",
    "model.resize_token_embeddings(len(text_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_form(template):\n",
    "    user = {\n",
    "        \"content\": f\"{template[0]}\\n{template[1]}\",\n",
    "        \"role\": \"user\"\n",
    "    }\n",
    "\n",
    "    assistant = {\n",
    "        \"content\": template[2],\n",
    "        \"role\": \"assistant\"\n",
    "    }\n",
    "\n",
    "    msg = {\"text\": text_tokenizer.apply_chat_template([user, assistant], add_generation_prompt=False, tokenize=False)}\n",
    "    return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symptoms = [\"diagnostic\", \"form\", \"rhythm\"]\n",
    "\n",
    "def create_message_column(row):\n",
    "    messages = []\n",
    "    ecg_text = [f\"<|ecg_{row.tokens[i][0]+1}|>\" for i in range(len(row.tokens))]\n",
    "    ecg_input = \"<|ecg_start|> \" + \" \".join(ecg_text) + \" <|ecg_end|>\"\n",
    "    age = int(row.age)\n",
    "    sex = row.sex\n",
    "\n",
    "    symptom_dict_list = [ast.literal_eval(row[symptoms[idx]]) for idx in range(3)]\n",
    "    S = Sentences.get_template(age, sex, ecg_input, symptom_dict_list)\n",
    "    C = Choices.get_template(age, sex, ecg_input, symptom_dict_list)\n",
    "    R = Reports.get_template(age, sex, ecg_input, symptom_dict_list)\n",
    "    P = Predict.get_template(age, sex, ecg_input)\n",
    "    lists = S + C + R + P\n",
    "    messages = [message_form(l) for l in lists]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_chatml = combined_df.apply(create_message_column, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_fold = 9\n",
    "test_fold = 10\n",
    "train_list = dataset_chatml[(combined_df.strat_fold != valid_fold) & (combined_df.strat_fold != test_fold)]\n",
    "valid_list = dataset_chatml[combined_df.strat_fold == valid_fold]\n",
    "test_list = dataset_chatml[combined_df.strat_fold == test_fold]\n",
    "\n",
    "train_dataset = Dataset.from_list([item for sublist in train_list for item in sublist]).shuffle(seed=42)\n",
    "valid_dataset = Dataset.from_list([item for sublist in valid_list for item in sublist])\n",
    "test_dataset = Dataset.from_list([item for sublist in test_list for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=valid_dataset,\n",
    "        peft_config=peft_config,\n",
    "        processing_class=text_tokenizer,\n",
    "        args=args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del trainer\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.empty_cache()\n",
    "    device = torch.device('cuda')\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "    print(\"CUDA 设备已重置\")\n",
    "else:\n",
    "    print(\"CUDA 不可用\")\n",
    "    \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "new_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    args.output_dir,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "merged_model = new_model.merge_and_unload()\n",
    "merged_model.save_pretrained(merged_model_path, trust_remote_code=True, safe_serialization=True)\n",
    "text_tokenizer.save_pretrained(merged_model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
